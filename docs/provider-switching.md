# âš¡ Provider & Model Switching

Zeke.nvim's revolutionary switching system lets you seamlessly move between AI providers and models without interrupting your workflow.

## ğŸ›ï¸ Quick Switcher

### Instant Access
```vim
<leader>zs  " Quick switcher popup
:ZekeSwitcher
```

**Features:**
- **Tab-based interface** - Providers tab and Models tab
- **Real-time status** - See current provider and model
- **Instant switching** - No restart required
- **Keyboard navigation** - Arrow keys, Tab, Enter

### Navigation
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          âš¡ Quick Switcher          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ğŸ”Œ Providers]   ğŸ¤– Models        â”‚
â”‚                                     â”‚
â”‚ ğŸ“ Current: openai â†’ gpt-4         â”‚
â”‚                                     â”‚
â”‚ ğŸ”Œ Available Providers:             â”‚
â”‚ â–¶ â­ ğŸ§  OpenAI (4 models)           â”‚
â”‚      ğŸ¤– Anthropic (3 models)       â”‚
â”‚      ğŸ  Ollama (8 models)           â”‚
â”‚      ğŸ™ GitHub Copilot (1 model)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Controls:**
- `â†‘/â†“` - Navigate items
- `Tab` - Switch between Provider/Model tabs
- `Enter` - Select item
- `r` - Refresh status
- `q/Esc` - Close

## ğŸ”„ Cycling Commands

### Provider Cycling
```vim
<leader>zp           " Cycle through providers
:ZekeCycleProvider   " Command version
```

**Cycle order:**
1. GhostLLM (intelligent routing)
2. GitHub Copilot (code completion)
3. OpenAI (general purpose)
4. Anthropic (reasoning)
5. Ollama (local/privacy)

### Model Cycling
```vim
<leader>zm         " Cycle through models
:ZekeCycleModel    " Command version
```

**Per-provider models:**
- **OpenAI**: gpt-4 â†’ gpt-4-turbo â†’ gpt-3.5-turbo
- **Anthropic**: claude-3-opus â†’ claude-3-sonnet â†’ claude-3-haiku
- **Ollama**: llama3:8b â†’ deepseek-coder â†’ codellama â†’ mistral
- **GitHub**: copilot (single model)

## ğŸ¯ Direct Switching

### Provider Commands
```vim
:ZekeSwitchProvider       " Open provider selector
:ZekeSetProvider openai   " Switch directly to OpenAI
:ZekeSetProvider anthropic " Switch directly to Anthropic
:ZekeSetProvider ollama   " Switch directly to Ollama
:ZekeSetProvider github   " Switch directly to GitHub Copilot
```

### Model Commands
```vim
:ZekeSwitchModel          " Open model selector
:ZekeSetModel gpt-4       " Switch to GPT-4
:ZekeSetModel claude-3-sonnet " Switch to Claude Sonnet
:ZekeSetModel llama3:8b   " Switch to Llama 3
```

## ğŸŒ GhostLLM Integration

### Intelligent Routing
When using GhostLLM proxy, specify "auto" model for intelligent routing:

```vim
:ZekeSetProvider ghostllm
:ZekeSetModel auto
```

**Routing logic:**
- **Code completion** â†’ GitHub Copilot or DeepSeek Coder
- **Complex reasoning** â†’ Claude 3 Opus
- **Quick tasks** â†’ GPT-3.5 Turbo or Llama 3
- **Cost-sensitive** â†’ Local Ollama models first

### Fallback Chain
```lua
-- Configure fallback providers
routing = {
  primary = "auto",
  fallback = {"ollama:llama3", "openai:gpt-3.5-turbo", "anthropic:claude-3-haiku"}
}
```

## ğŸª Advanced Features

### Context-Aware Switching
Different tasks automatically suggest optimal providers:

```vim
:ZekeExplain        " Suggests: Claude (reasoning)
:ZekeEdit           " Suggests: GPT-4 (editing)
:ZekeAnalyze        " Suggests: Claude (analysis)
:ZekeCreate         " Suggests: Copilot (code gen)
```

### Smart Suggestions
The switcher shows recommendations:
```
ğŸ”Œ Recommended for current task:
  â­ Claude 3 Sonnet (best for code explanation)
  âœ… GPT-4 (good general purpose)
  ğŸ  Llama 3 (privacy-focused local)
```

### Session Memory
```vim
" Remembers your last choice per context
:ZekeChat "explain this"     " Uses Claude (last reasoning choice)
:ZekeEdit "fix the bug"      " Uses GPT-4 (last editing choice)
:ZekeCreate "new component"  " Uses Copilot (last creation choice)
```

## ğŸ“Š Status Integration

### Status Line
Add to your status line to see current provider:
```lua
-- Status line integration
function ZekeStatus()
  local zeke = require('zeke')
  return zeke.get_status_line()  -- "âš¡ openai:gpt-4"
end
```

### Visual Indicators
```vim
:ZekeStatus  " Show detailed connection status
```

Output:
```
âš¡ Zeke Status:
â”œâ”€ Provider: openai (âœ… authenticated)
â”œâ”€ Model: gpt-4
â”œâ”€ Connection: WebSocket connected
â”œâ”€ Session: zeke-cli-abc123
â””â”€ Response time: 245ms
```

## ğŸ”§ Configuration

### Default Providers
```lua
require("zeke").setup({
  -- Provider preference order
  provider_priority = {
    "ghostllm",    -- Intelligent routing first
    "github",      -- Copilot for code
    "anthropic",   -- Claude for reasoning
    "openai",      -- GPT for general
    "ollama"       -- Local for privacy
  },

  -- Model preferences per provider
  preferred_models = {
    openai = "gpt-4",
    anthropic = "claude-3-sonnet",
    ollama = "llama3:8b",
    github = "copilot"
  },

  -- Task-specific routing
  routing = {
    code_completion = "github",
    explanation = "anthropic",
    editing = "openai",
    analysis = "anthropic",
    creation = "github"
  }
})
```

### Switching Behavior
```lua
-- Customize switching behavior
switching = {
  confirm_expensive = true,      -- Confirm before using costly models
  auto_fallback = true,         -- Fall back if provider fails
  preserve_context = true,      -- Keep context when switching
  show_cost_estimates = true,   -- Show cost per request
}
```

## ğŸš¨ Cost Management

### Cost Awareness
```vim
" Shows estimated cost before switching
:ZekeSetModel gpt-4
# Warning: GPT-4 costs ~$0.03/request. Continue? [y/N]

:ZekeSetModel llama3:8b
# âœ… Local model - no API costs
```

### Usage Tracking
```vim
:ZekeProviders  " Show cost breakdown per provider
```

Display:
```
ğŸ’° Today's Usage:
â”œâ”€ OpenAI: $2.40 (12 requests)
â”œâ”€ Anthropic: $1.80 (6 requests)
â”œâ”€ GitHub: $0.00 (Pro subscription)
â””â”€ Ollama: $0.00 (local)
```

## ğŸ”„ Live Switching During Conversations

### Mid-Conversation Switching
```vim
" Start with one provider
:ZekeChat "Explain async Rust"

" Switch provider mid-conversation
<leader>zs  " Select different provider
" Context automatically transfers!
```

### Provider Comparison
```vim
" Ask the same question to multiple providers
:ZekeChat "Optimize this function"
<leader>zp  " Cycle to next provider
:ZekeChat "same"  " Reuse last question
```

## ğŸ¯ Best Practices

### Optimal Provider Selection
- **ğŸ“ Code completion** â†’ GitHub Copilot
- **ğŸ§  Complex reasoning** â†’ Claude 3 Opus/Sonnet
- **âš¡ Quick edits** â†’ GPT-3.5 Turbo
- **ğŸ”’ Sensitive code** â†’ Local Ollama
- **ğŸ’° Cost-sensitive** â†’ Ollama > GPT-3.5 > Claude Haiku

### Workflow Examples
```vim
" Privacy-first development
<leader>zs â†’ Ollama â†’ llama3:8b

" Maximum capability
<leader>zs â†’ Anthropic â†’ claude-3-opus

" Balanced cost/performance
<leader>zs â†’ GhostLLM â†’ auto

" Code-focused session
<leader>zs â†’ GitHub â†’ copilot
```

### Keyboard Shortcuts Summary
```vim
<leader>zs  # Quick switcher (recommended)
<leader>zp  # Cycle providers
<leader>zm  # Cycle models
<leader>za  # Authentication management
<leader>zP  # Full provider management UI
```

Ready to experience instant AI provider switching? Press `<leader>zs` and switch at the speed of thought! âš¡